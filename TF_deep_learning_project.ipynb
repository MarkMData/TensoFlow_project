{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1d6f32fe-8b19-45ad-8ab2-7e91ff95dafc"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "np.set_printoptions(suppress=True)\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "import datetime"
      ],
      "id": "1d6f32fe-8b19-45ad-8ab2-7e91ff95dafc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6114aad9-5ea8-4b8f-8d14-dc0a0d3f1b62"
      },
      "source": [
        "### Pre-processing"
      ],
      "id": "6114aad9-5ea8-4b8f-8d14-dc0a0d3f1b62"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vtlWFfOInAP9"
      },
      "outputs": [],
      "source": [
        "# specify the data file name and url\n",
        "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00240/'\n",
        "datafile = url + 'UCI%20HAR%20Dataset.zip'\n",
        "# download the zip file from the web server using curl\n",
        "!curl $datafile --output UCI_HAR_Dataset.zip\n",
        "# unzip the file\n",
        "!unzip -qq UCI_HAR_Dataset.zip\n",
        "# change the directory name to remove spaces\n",
        "!mv -f UCI\\ HAR\\ Dataset UCI_HAR_DATASET\n"
      ],
      "id": "vtlWFfOInAP9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "900c2d41-83cb-45a4-b397-070e552fe980"
      },
      "outputs": [],
      "source": [
        "# load the features and labels (subtract 1 as the labels aren't indexed from 0)\n",
        "ytest = np.loadtxt('UCI_HAR_DATASET/test/y_test.txt')-1\n",
        "ytrain = np.loadtxt('UCI_HAR_DATASET/train/y_train.txt')-1\n",
        "# load the x,y,z body accelerations test data\n",
        "xx=np.loadtxt('UCI_HAR_DATASET/test/Inertial Signals/body_acc_x_test.txt')\n",
        "yy=np.loadtxt('UCI_HAR_DATASET/test/Inertial Signals/body_acc_y_test.txt')\n",
        "zz=np.loadtxt('UCI_HAR_DATASET/test/Inertial Signals/body_acc_z_test.txt')\n",
        "# concatenate the arrays along the last dimension\n",
        "xtest = np.concatenate((xx[:,:,None],yy[:,:,None],zz[:,:,None]),axis=2)\n",
        "# (using None here adds an extra dimension of size 1 to the end of the array)\n",
        "# follow the same approach for the train data\n",
        "xx=np.loadtxt('UCI_HAR_DATASET/train/Inertial Signals/body_acc_x_train.txt')\n",
        "yy=np.loadtxt('UCI_HAR_DATASET/train/Inertial Signals/body_acc_y_train.txt')\n",
        "zz=np.loadtxt('UCI_HAR_DATASET/train/Inertial Signals/body_acc_z_train.txt')\n",
        "xtrain = np.concatenate((xx[:,:,None],yy[:,:,None],zz[:,:,None]),axis=2)\n"
      ],
      "id": "900c2d41-83cb-45a4-b397-070e552fe980"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ad883bee-ba2b-49b2-ad92-74ef52cc78e3"
      },
      "source": [
        "# Part 1."
      ],
      "id": "ad883bee-ba2b-49b2-ad92-74ef52cc78e3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWHpC8TeMGc3"
      },
      "outputs": [],
      "source": [
        "# Splitting training data into train and validation sets\n",
        "x_train, x_val, y_train, y_val = train_test_split(\n",
        "    xtrain, ytrain, test_size = 0.2, random_state=42)\n",
        "\n",
        "# Re-shaping training data tensor\n",
        "x_train_flat = tf.cast(tf.reshape(x_train, (-1,128*3)), tf.float32)\n",
        "x_val_flat = tf.cast(tf.reshape(x_val, (-1,128*3)), tf.float32)\n",
        "x_test_flat = tf.cast(tf.reshape(xtest, (-1,128*3)), tf.float32)\n",
        "\n",
        "# Changing labels to one-hot encoding\n",
        "y_train1h = tf.cast(tf.keras.utils.to_categorical(y_train), tf.float32)\n",
        "y_val1h = tf.cast(tf.keras.utils.to_categorical(y_val), tf.float32)\n",
        "y_test1h = tf.cast(tf.keras.utils.to_categorical(ytest), tf.float32)\n",
        "\n",
        "# Checking shape of data\n",
        "print(ytest.shape)\n",
        "print(ytrain.shape)\n",
        "print(xtest.shape)\n",
        "print(xtrain.shape)\n",
        "print(x_train_flat.shape)\n",
        "print(x_val_flat.shape)\n",
        "print(y_train1h.shape)\n",
        "print(y_val1h.shape)"
      ],
      "id": "RWHpC8TeMGc3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9643307-2b86-4cbd-b41e-63d9792e053b"
      },
      "source": [
        "### Setting up simple multinomial regression model"
      ],
      "id": "e9643307-2b86-4cbd-b41e-63d9792e053b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "817a4b8e-b114-4ba6-99f1-61df6bea9dc6"
      },
      "outputs": [],
      "source": [
        "# predicted probability for each class\n",
        "def predict_function(x):\n",
        "    return tf.nn.softmax(tf.matmul(x,W) + b)\n",
        "\n",
        "# cross entropy loss function\n",
        "def loss(y_pred,y):\n",
        "    y_pred = tf.clip_by_value(y_pred, 1e-9, 1.)\n",
        "    return tf.reduce_mean(-tf.reduce_sum(y * tf.math.log(y_pred), axis=[1]))\n",
        "\n",
        "# Defining training function\n",
        "def train_function(x, y):\n",
        "    with tf.GradientTape() as tape:\n",
        "        predicted = predict_function(x)\n",
        "        current_loss = loss(predicted, y)\n",
        "    gradients = tape.gradient(current_loss, [W, b])\n",
        "    optimizer.apply_gradients(zip(gradients, [W ,b]))\n",
        "    return current_loss\n",
        "\n",
        "# Defining accuracy function\n",
        "def accuracy(x,y):\n",
        "    y_ = predict_function(x)\n",
        "    # calculate where the prediction equals the label\n",
        "    correct = tf.math.equal(tf.math.argmax(y_,axis=-1),tf.math.argmax(y,axis=-1))\n",
        "    # convert to a float (previously boolean)\n",
        "    correct = tf.cast(correct,dtype=tf.float32)\n",
        "    # return the mean to give the overall accuracy\n",
        "    return tf.math.reduce_mean(correct)"
      ],
      "id": "817a4b8e-b114-4ba6-99f1-61df6bea9dc6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6f2fcc00-13bf-4f4d-be3b-d1493ca33eb4"
      },
      "source": [
        "### Running training"
      ],
      "id": "6f2fcc00-13bf-4f4d-be3b-d1493ca33eb4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJkJPwpH_ti6",
        "outputId": "a9ce7141-c1f6-4018-ff17-c73e1dd4c0ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ]
        }
      ],
      "source": [
        "%load_ext tensorboard"
      ],
      "id": "JJkJPwpH_ti6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ee0c16a9-694d-4df8-84d8-7667bb6ff61a"
      },
      "outputs": [],
      "source": [
        "# Create tensorflow graph\n",
        "W = tf.Variable(tf.zeros([384, 6]))\n",
        "b = tf.Variable(tf.zeros([6]))\n",
        "\n",
        "# Set up logging\n",
        "logdir = 'tflogs'\n",
        "writer = tf.summary.create_file_writer(logdir)\n",
        "\n",
        "epochs = 20000\n",
        "lr = 0.001\n",
        "optimizer = tf.keras.optimizers.Adam(lr)\n",
        "loss_history = np.zeros(epochs)\n",
        "\n",
        "tf.summary.trace_on()\n",
        "\n",
        "with writer.as_default():\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        training_loss = train_function(x_train_flat, y_train1h)\n",
        "        train_acc = accuracy(x_train_flat, y_train1h)\n",
        "        valid_acc = accuracy(x_val_flat, y_val1h)\n",
        "        tf.summary.scalar('Model1_loss', training_loss, step = epoch)\n",
        "        tf.summary.scalar('Model1_train_accuracy', train_acc, step = epoch)\n",
        "        tf.summary.scalar('Model1_valid_accuracy', valid_acc, step = epoch)\n"
      ],
      "id": "ee0c16a9-694d-4df8-84d8-7667bb6ff61a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehQ6RUVwA4i4"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir /content/tflogs"
      ],
      "id": "ehQ6RUVwA4i4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50c857c4-cd0a-4e5b-b76a-d90fb6c545a8"
      },
      "source": [
        "### Looking at prediction accuracy"
      ],
      "id": "50c857c4-cd0a-4e5b-b76a-d90fb6c545a8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "595bb772-5eec-4b66-8baa-edb49fd6416b"
      },
      "outputs": [],
      "source": [
        "# Training accuracy\n",
        "print(accuracy(x_train_flat, y_train1h))\n",
        "# Validation accuracy\n",
        "print(accuracy(x_val_flat, y_val1h))"
      ],
      "id": "595bb772-5eec-4b66-8baa-edb49fd6416b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9657eed7-1912-4a47-beaa-ef37486961d9"
      },
      "outputs": [],
      "source": [
        "# NOTE: THIS HAS BEEN PUT HERE TO SAVE RE-RUNNING THE MODEL LATER\n",
        "# Model from Part 1 test accuracy\n",
        "accuracy(x_test_flat, y_test1h)"
      ],
      "id": "9657eed7-1912-4a47-beaa-ef37486961d9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10576d41-6ced-4fbc-a842-876d61fb2a1e"
      },
      "source": [
        "# Part 2."
      ],
      "id": "10576d41-6ced-4fbc-a842-876d61fb2a1e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acc9c07a-be40-42f9-890a-39193a05a8e3"
      },
      "source": [
        "### Creating function for training 1-D convolutional network"
      ],
      "id": "acc9c07a-be40-42f9-890a-39193a05a8e3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43c786c9-9516-44a8-b6a5-c7b36c9418db"
      },
      "outputs": [],
      "source": [
        "# Setting up function for training network with a single 1-D convolutional layer\n",
        "def network2(kernel, filters, lr, batch_size, epochs):\n",
        "    # Constructing CNN model\n",
        "    cnn1 = tf.keras.Sequential()\n",
        "    cnn1.add(tf.keras.layers.Conv1D(filters = filters, kernel_size = kernel, input_shape = (128, 3)))\n",
        "    cnn1.add(tf.keras.layers.BatchNormalization())\n",
        "    cnn1.add(tf.keras.layers.ReLU())\n",
        "    cnn1.add(tf.keras.layers.GlobalAveragePooling1D())\n",
        "    cnn1.add(tf.keras.layers.Dense(units = 6, activation = tf.nn.softmax))\n",
        "    # Set up logging\n",
        "    logdir = 'tflogs'\n",
        "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
        "    # creating callback to save best model\n",
        "    callbacks = [\n",
        "        tf.keras.callbacks.ModelCheckpoint(\"best_model_cnn1\", save_best_only=True, monitor=\"val_accuracy\",\n",
        "                                           mode='max', verbose = 0),\n",
        "        tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
        "    ]\n",
        "    # Compiling\n",
        "    cnn1.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = lr),\n",
        "    \t\t\tloss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    # Fitting\n",
        "    cnn1.fit(x_train, y_train1h, epochs = epochs, batch_size = batch_size, verbose = 0,\n",
        "            validation_data=(x_val, y_val1h), callbacks=callbacks)\n"
      ],
      "id": "43c786c9-9516-44a8-b6a5-c7b36c9418db"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bcd113c-701c-4cc4-a5e9-29cd6c72dd90"
      },
      "source": [
        "### Trying different learning rates"
      ],
      "id": "3bcd113c-701c-4cc4-a5e9-29cd6c72dd90"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wst3xk-2CzGZ"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir /content/tflogs"
      ],
      "id": "Wst3xk-2CzGZ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ff82148b-866e-4235-9d39-076c2f6e1298"
      },
      "outputs": [],
      "source": [
        "# Remove the previous log folder\n",
        "!rm -rf ./tflogs/\n",
        "network2(kernel=4, filters=32, lr=0.01, batch_size=32, epochs=2000)\n",
        "# recording best model configuration\n",
        "net1_1 = tf.keras.models.load_model(\"best_model_cnn1\")\n",
        "# Training accuracy\n",
        "print(\"Training loss and accuracy:\", net1_1.evaluate(x_train, y_train1h))\n",
        "# Validation accuracy\n",
        "print(\"Validation loss and accuracy:\", net1_1.evaluate(x_val, y_val1h))"
      ],
      "id": "ff82148b-866e-4235-9d39-076c2f6e1298"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d845ba3c-dbf3-49eb-a139-cbebebe131d8"
      },
      "outputs": [],
      "source": [
        "# Remove the previous log folder\n",
        "!rm -rf ./tflogs/\n",
        "network2(kernel=4, filters=32, lr=0.001, batch_size=32, epochs=2000)\n",
        "# recording best model configuration\n",
        "net1_2 = tf.keras.models.load_model(\"best_model_cnn1\")\n",
        "# Training accuracy\n",
        "print(\"Training loss and accuracy:\", net1_2.evaluate(x_train, y_train1h))\n",
        "# Validation accuracy\n",
        "print(\"Validation loss and accuracy:\", net1_2.evaluate(x_val, y_val1h))"
      ],
      "id": "d845ba3c-dbf3-49eb-a139-cbebebe131d8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1b6501e-5b04-4705-a943-b3ea971acc7b"
      },
      "outputs": [],
      "source": [
        "# Remove the previous log folder\n",
        "!rm -rf ./tflogs/\n",
        "network2(kernel=4, filters=32, lr=0.0001, batch_size=32, epochs=2000)\n",
        "# recording best model configuration\n",
        "net1_3 = tf.keras.models.load_model(\"best_model_cnn1\")\n",
        "# Training accuracy\n",
        "print(\"Training loss and accuracy:\", net1_3.evaluate(x_train, y_train1h))\n",
        "# Validation accuracy\n",
        "print(\"Validation loss and accuracy:\", net1_3.evaluate(x_val, y_val1h))"
      ],
      "id": "f1b6501e-5b04-4705-a943-b3ea971acc7b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d49168cc-85c1-4c09-8509-7dd75f04e454"
      },
      "source": [
        "### Testing different batch sizes"
      ],
      "id": "d49168cc-85c1-4c09-8509-7dd75f04e454"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQsArKghBQpQ"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir /content/tflogs"
      ],
      "id": "UQsArKghBQpQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dcabe21b-1513-474b-a341-88913e15cfd1"
      },
      "outputs": [],
      "source": [
        "# Remove the previous log folder\n",
        "!rm -rf ./tflogs/\n",
        "# Network 2 with batch size 32\n",
        "network2(kernel=4, filters=32, lr=0.0001, batch_size=32, epochs=2000)\n",
        "# recording best model configuration\n",
        "net2_1 = tf.keras.models.load_model(\"best_model_cnn1\")\n",
        "# Training accuracy\n",
        "print(\"Training loss and accuracy:\", net2_1.evaluate(x_train, y_train1h))\n",
        "# Validation accuracy\n",
        "print(\"Validation loss and accuracy:\", net2_1.evaluate(x_val, y_val1h))"
      ],
      "id": "dcabe21b-1513-474b-a341-88913e15cfd1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ddf6460d-6cd9-4898-8350-7f2155ed1d4e"
      },
      "outputs": [],
      "source": [
        "# Remove the previous log folder\n",
        "!rm -rf ./tflogs/\n",
        "# Network 2 with batch size 64\n",
        "network2(kernel=4, filters=32, lr=0.0001, batch_size=64, epochs=2000)\n",
        "# recording best model configuration\n",
        "net2_2 = tf.keras.models.load_model(\"best_model_cnn1\")\n",
        "# Training accuracy\n",
        "print(\"Training loss and accuracy:\", net2_2.evaluate(x_train, y_train1h))\n",
        "# Validation accuracy\n",
        "print(\"Validation loss and accuracy:\", net2_2.evaluate(x_val, y_val1h))"
      ],
      "id": "ddf6460d-6cd9-4898-8350-7f2155ed1d4e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0013cb4-9b3c-4a9b-9041-791b2cacf143"
      },
      "outputs": [],
      "source": [
        "# Remove the previous log folder\n",
        "!rm -rf ./tflogs/\n",
        "# Network 2 with batch size 128\n",
        "network2(kernel=4, filters=32, lr=0.0001, batch_size=128, epochs=2000)\n",
        "# recording best model configuration\n",
        "net2_3 = tf.keras.models.load_model(\"best_model_cnn1\")\n",
        "# Training accuracy\n",
        "print(\"Training loss and accuracy:\", net2_3.evaluate(x_train, y_train1h))\n",
        "# Validation accuracy\n",
        "print(\"Validation loss and accuracy:\", net2_3.evaluate(x_val, y_val1h))"
      ],
      "id": "e0013cb4-9b3c-4a9b-9041-791b2cacf143"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f01cc0e2-2f2d-43a8-814d-bac9dd347cd4"
      },
      "source": [
        "### Testing different network parameters"
      ],
      "id": "f01cc0e2-2f2d-43a8-814d-bac9dd347cd4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2c519e0-d386-49d1-a5e0-d5a03b1fcd06"
      },
      "source": [
        "Different kernal sizes with 16 filters"
      ],
      "id": "e2c519e0-d386-49d1-a5e0-d5a03b1fcd06"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78iAwEtJBaXC"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir /content/tflogs"
      ],
      "id": "78iAwEtJBaXC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d12e87fd-096b-467a-a7c5-8ca245e6c9d8"
      },
      "outputs": [],
      "source": [
        "# Remove the previous log folder\n",
        "!rm -rf ./tflogs/\n",
        "network2(kernel=3, filters=16, lr=0.0001, batch_size=64, epochs=2000)\n",
        "# recording best model configuration\n",
        "net2_4 = tf.keras.models.load_model(\"best_model_cnn1\")\n",
        "# Training accuracy\n",
        "print(\"Training loss and accuracy:\", net2_4.evaluate(x_train, y_train1h))\n",
        "# Validation accuracy\n",
        "print(\"Validation loss and accuracy:\", net2_4.evaluate(x_val, y_val1h))"
      ],
      "id": "d12e87fd-096b-467a-a7c5-8ca245e6c9d8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b79fa65c-f864-4981-b609-220f0ae2c49e"
      },
      "outputs": [],
      "source": [
        "# Remove the previous log folder\n",
        "!rm -rf ./tflogs/\n",
        "network2(kernel=4, filters=16, lr=0.0001, batch_size=64, epochs=2000)\n",
        "# recording best model configuration\n",
        "net2_5 = tf.keras.models.load_model(\"best_model_cnn1\")\n",
        "# Training accuracy\n",
        "print(\"Training loss and accuracy:\", net2_5.evaluate(x_train, y_train1h))\n",
        "# Validation accuracy\n",
        "print(\"Validation loss and accuracy:\", net2_5.evaluate(x_val, y_val1h))"
      ],
      "id": "b79fa65c-f864-4981-b609-220f0ae2c49e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9291ec88-d2cc-4f4a-a7d8-0a564e450de0"
      },
      "outputs": [],
      "source": [
        "# Remove the previous log folder\n",
        "!rm -rf ./tflogs/\n",
        "network2(kernel=5, filters=16, lr=0.0001, batch_size=64, epochs=2000)\n",
        "# recording best model configuration\n",
        "net2_6 = tf.keras.models.load_model(\"best_model_cnn1\")\n",
        "# Training accuracy\n",
        "print(\"Training loss and accuracy:\", net2_6.evaluate(x_train, y_train1h))\n",
        "# Validation accuracy\n",
        "print(\"Validation loss and accuracy:\", net2_6.evaluate(x_val, y_val1h))"
      ],
      "id": "9291ec88-d2cc-4f4a-a7d8-0a564e450de0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3528bce7-ebb0-4eaf-9de8-20b5595ae9f6"
      },
      "source": [
        "Different kernal sizes with 32 filters"
      ],
      "id": "3528bce7-ebb0-4eaf-9de8-20b5595ae9f6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGQVa8ERBbgD"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir /content/tflogs"
      ],
      "id": "gGQVa8ERBbgD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3edaae24-cff3-44c7-ba6f-bc3c9643e70f"
      },
      "outputs": [],
      "source": [
        "# Remove the previous log folder\n",
        "!rm -rf ./tflogs/\n",
        "network2(kernel=3, filters=32, lr=0.0001, batch_size=64, epochs=2000)\n",
        "# recording best model configuration\n",
        "net2_7 = tf.keras.models.load_model(\"best_model_cnn1\")\n",
        "# Training accuracy\n",
        "print(\"Training loss and accuracy:\", net2_7.evaluate(x_train, y_train1h))\n",
        "# Validation accuracy\n",
        "print(\"Validation loss and accuracy:\", net2_7.evaluate(x_val, y_val1h))"
      ],
      "id": "3edaae24-cff3-44c7-ba6f-bc3c9643e70f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "557ab3d4-6ad8-47f2-a79d-f133bb40f401"
      },
      "outputs": [],
      "source": [
        "# Remove the previous log folder\n",
        "!rm -rf ./tflogs/\n",
        "network2(kernel=4, filters=32, lr=0.0001, batch_size=64, epochs=2000)\n",
        "# recording best model configuration\n",
        "net2_8 = tf.keras.models.load_model(\"best_model_cnn1\")\n",
        "# Training accuracy\n",
        "print(\"Training loss and accuracy:\", net2_8.evaluate(x_train, y_train1h))\n",
        "# Validation accuracy\n",
        "print(\"Validation loss and accuracy:\", net2_8.evaluate(x_val, y_val1h))"
      ],
      "id": "557ab3d4-6ad8-47f2-a79d-f133bb40f401"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b91f47f4-6bd3-432f-b8b2-bbe2e1e243b5"
      },
      "outputs": [],
      "source": [
        "# Remove the previous log folder\n",
        "!rm -rf ./tflogs/\n",
        "network2(kernel=5, filters=32, lr=0.0001, batch_size=64, epochs=2000)\n",
        "# recording best model configuration\n",
        "net2_9 = tf.keras.models.load_model(\"best_model_cnn1\")\n",
        "# Training accuracy\n",
        "print(\"Training loss and accuracy:\", net2_9.evaluate(x_train, y_train1h))\n",
        "# Validation accuracy\n",
        "print(\"Validation loss and accuracy:\", net2_9.evaluate(x_val, y_val1h))"
      ],
      "id": "b91f47f4-6bd3-432f-b8b2-bbe2e1e243b5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "458d9d5f-dc16-403a-b05b-9c9a1b745525"
      },
      "source": [
        "Different kernal sizes with 64 filters"
      ],
      "id": "458d9d5f-dc16-403a-b05b-9c9a1b745525"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVVlV9J8Bcwk"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir /content/tflogs"
      ],
      "id": "ZVVlV9J8Bcwk"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ec52035e-948b-4213-9821-367484f71ad8"
      },
      "outputs": [],
      "source": [
        "# Remove the previous log folder\n",
        "!rm -rf ./tflogs/\n",
        "network2(kernel=3, filters=64, lr=0.0001, batch_size=64, epochs=2000)\n",
        "# recording best model configuration\n",
        "net2_10 = tf.keras.models.load_model(\"best_model_cnn1\")\n",
        "# Training accuracy\n",
        "print(\"Training loss and accuracy:\", net2_10.evaluate(x_train, y_train1h))\n",
        "# Validation accuracy\n",
        "print(\"Validation loss and accuracy:\", net2_10.evaluate(x_val, y_val1h))"
      ],
      "id": "ec52035e-948b-4213-9821-367484f71ad8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43ef9879-5cb1-4b24-9b76-e615db4b79ff"
      },
      "outputs": [],
      "source": [
        "# Remove the previous log folder\n",
        "!rm -rf ./tflogs/\n",
        "network2(kernel=4, filters=64, lr=0.0001, batch_size=64, epochs=2000)\n",
        "# recording best model configuration\n",
        "net2_11 = tf.keras.models.load_model(\"best_model_cnn1\")\n",
        "# Training accuracy\n",
        "print(\"Training loss and accuracy:\", net2_11.evaluate(x_train, y_train1h))\n",
        "# Validation accuracy\n",
        "print(\"Validation loss and accuracy:\", net2_11.evaluate(x_val, y_val1h))"
      ],
      "id": "43ef9879-5cb1-4b24-9b76-e615db4b79ff"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7404cdfd-4ccd-48e7-bcbf-d1c0718169e1"
      },
      "outputs": [],
      "source": [
        "# NOTE: THIS HAS BEEN PUT HERE TO SAVE RE-RUNNING THE MODEL LATER\n",
        "# Model from Part 2 test accuracy\n",
        "print(\"Test loss and accuracy:\", net2_11.evaluate(xtest, y_test1h))"
      ],
      "id": "7404cdfd-4ccd-48e7-bcbf-d1c0718169e1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2d3dab8-2832-45b8-8685-f53b51037f07"
      },
      "outputs": [],
      "source": [
        "# Remove the previous log folder\n",
        "!rm -rf ./tflogs/\n",
        "network2(kernel=5, filters=64, lr=0.0001, batch_size=64, epochs=2000)\n",
        "# recording best model configuration\n",
        "net2_12 = tf.keras.models.load_model(\"best_model_cnn1\")\n",
        "# Training accuracy\n",
        "print(\"Training loss and accuracy:\", net2_12.evaluate(x_train, y_train1h))\n",
        "# Validation accuracy\n",
        "print(\"Validation loss and accuracy:\", net2_12.evaluate(x_val, y_val1h))"
      ],
      "id": "b2d3dab8-2832-45b8-8685-f53b51037f07"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4e2024cf-5fea-4a1b-b3eb-5112055298f2"
      },
      "source": [
        "### Trying network with additional layers"
      ],
      "id": "4e2024cf-5fea-4a1b-b3eb-5112055298f2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "594e5e6a-e739-4cf9-abe6-2ea50c71253f"
      },
      "outputs": [],
      "source": [
        "# Trying two convolutional layers. First layer with a smaller kernal and less filters,\n",
        "# second layer larger kernal more filters. Batch normalization after each.\n",
        "# Parameters\n",
        "# Remove the previous log folder\n",
        "!rm -rf ./tflogs/\n",
        "lr = 0.0001\n",
        "batch_size = 64\n",
        "epochs = 2000\n",
        "# Constructing CNN model\n",
        "cnn1 = tf.keras.Sequential()\n",
        "cnn1.add(tf.keras.layers.Conv1D(filters = 32, kernel_size = 3, input_shape = (128, 3)))\n",
        "cnn1.add(tf.keras.layers.BatchNormalization())\n",
        "cnn1.add(tf.keras.layers.Conv1D(filters = 64, kernel_size = 4))\n",
        "cnn1.add(tf.keras.layers.BatchNormalization())\n",
        "cnn1.add(tf.keras.layers.ReLU())\n",
        "cnn1.add(tf.keras.layers.GlobalAveragePooling1D())\n",
        "cnn1.add(tf.keras.layers.Dense(units = 6, activation = tf.nn.softmax))\n",
        "# Set up logging\n",
        "logdir = 'tflogs'\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
        "# creating callback to save best model\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.ModelCheckpoint(\"best_model_cnn1\", save_best_only=True, monitor=\"val_accuracy\",\n",
        "                                       mode='max', verbose = 0),\n",
        "    tf.keras.callbacks.TensorBoard(log_dir=logdir)]\n",
        "# Compiling\n",
        "cnn1.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = lr),\n",
        "\t\t\tloss='categorical_crossentropy', metrics=['accuracy'])\n",
        "# Fitting\n",
        "cnn1.fit(x_train, y_train1h, epochs = epochs, batch_size = batch_size, verbose = 0,\n",
        "        validation_data=(x_val, y_val1h), callbacks=callbacks)\n",
        "# recording best model configuration\n",
        "net2_13 = tf.keras.models.load_model(\"best_model_cnn1\")\n",
        "# Training accuracy\n",
        "print(\"Training loss and accuracy:\", net2_13.evaluate(x_train, y_train1h))\n",
        "# Validation accuracy\n",
        "print(\"Validation loss and accuracy:\", net2_13.evaluate(x_val, y_val1h))"
      ],
      "id": "594e5e6a-e739-4cf9-abe6-2ea50c71253f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGv3-R-3BgO6"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir /content/tflogs"
      ],
      "id": "CGv3-R-3BgO6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3cebb28c-844e-42a5-a512-314f4fc439a6"
      },
      "outputs": [],
      "source": [
        "# Trying three convolutional layers.\n",
        "# Remove the previous log folder\n",
        "!rm -rf ./tflogs/\n",
        "# Parameters\n",
        "lr = 0.0001\n",
        "batch_size = 64\n",
        "epochs = 2000\n",
        "# Constructing CNN model\n",
        "cnn1 = tf.keras.Sequential()\n",
        "cnn1.add(tf.keras.layers.Conv1D(filters = 16, kernel_size = 3, input_shape = (128, 3)))\n",
        "cnn1.add(tf.keras.layers.BatchNormalization())\n",
        "cnn1.add(tf.keras.layers.Conv1D(filters = 32, kernel_size = 4))\n",
        "cnn1.add(tf.keras.layers.BatchNormalization())\n",
        "cnn1.add(tf.keras.layers.Conv1D(filters = 64, kernel_size = 5))\n",
        "cnn1.add(tf.keras.layers.BatchNormalization())\n",
        "cnn1.add(tf.keras.layers.ReLU())\n",
        "cnn1.add(tf.keras.layers.GlobalAveragePooling1D())\n",
        "cnn1.add(tf.keras.layers.Dense(units = 6, activation = tf.nn.softmax))\n",
        "# Set up logging\n",
        "logdir = 'tflogs'\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
        "# creating callback to save best model\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.ModelCheckpoint(\"best_model_cnn1\", save_best_only=True, monitor=\"val_accuracy\",\n",
        "                                       mode='max', verbose = 0),\n",
        "    tf.keras.callbacks.TensorBoard(log_dir=logdir)]\n",
        "# Compiling\n",
        "cnn1.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = lr),\n",
        "\t\t\tloss='categorical_crossentropy', metrics=['accuracy'])\n",
        "# Fitting\n",
        "cnn1.fit(x_train, y_train1h, epochs = epochs, batch_size = batch_size, verbose = 0,\n",
        "        validation_data=(x_val, y_val1h), callbacks=callbacks)\n",
        "# recording best model configuration\n",
        "net2_14 = tf.keras.models.load_model(\"best_model_cnn1\")\n",
        "# Training accuracy\n",
        "print(\"Training loss and accuracy:\", net2_14.evaluate(x_train, y_train1h))\n",
        "# Validation accuracy\n",
        "print(\"Validation loss and accuracy:\", net2_14.evaluate(x_val, y_val1h))"
      ],
      "id": "3cebb28c-844e-42a5-a512-314f4fc439a6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3e4a1393-de36-4231-8a75-dd1c2f975d08"
      },
      "source": [
        "# Part 3."
      ],
      "id": "3e4a1393-de36-4231-8a75-dd1c2f975d08"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d7476be-3c3b-4841-945b-aa8d431ae0c2"
      },
      "source": [
        "### Load in the rest of the variables"
      ],
      "id": "2d7476be-3c3b-4841-945b-aa8d431ae0c2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f480c3af-d480-4cbe-9c2d-27fa30ce069b"
      },
      "outputs": [],
      "source": [
        "# load the x,y,z accelerations for the three data streams for each coordinate for test data\n",
        "xx=np.loadtxt('UCI_HAR_DATASET/test/Inertial Signals/body_acc_x_test.txt')\n",
        "yy=np.loadtxt('UCI_HAR_DATASET/test/Inertial Signals/body_acc_y_test.txt')\n",
        "zz=np.loadtxt('UCI_HAR_DATASET/test/Inertial Signals/body_acc_z_test.txt')\n",
        "xxt=np.loadtxt('UCI_HAR_DATASET/test/Inertial Signals/total_acc_x_test.txt')\n",
        "yyt=np.loadtxt('UCI_HAR_DATASET/test/Inertial Signals/total_acc_y_test.txt')\n",
        "zzt=np.loadtxt('UCI_HAR_DATASET/test/Inertial Signals/total_acc_z_test.txt')\n",
        "xxg=np.loadtxt('UCI_HAR_DATASET/test/Inertial Signals/body_gyro_x_test.txt')\n",
        "yyg=np.loadtxt('UCI_HAR_DATASET/test/Inertial Signals/body_gyro_y_test.txt')\n",
        "zzg=np.loadtxt('UCI_HAR_DATASET/test/Inertial Signals/body_gyro_z_test.txt')\n",
        "# concatenate into a single (n,128,9) array\n",
        "xtest_all = np.concatenate((xx[:,:,None],yy[:,:,None],zz[:,:,None],\n",
        "xxt[:,:,None],yyt[:,:,None],zzt[:,:,None],\n",
        "xxg[:,:,None],yyg[:,:,None],zzg[:,:,None]),axis=2)\n",
        "\n",
        "# load the x,y,z accelerations for the three data streams for each coordinate for train data\n",
        "xx=np.loadtxt('UCI_HAR_DATASET/train/Inertial Signals/body_acc_x_train.txt')\n",
        "yy=np.loadtxt('UCI_HAR_DATASET/train/Inertial Signals/body_acc_y_train.txt')\n",
        "zz=np.loadtxt('UCI_HAR_DATASET/train/Inertial Signals/body_acc_z_train.txt')\n",
        "xxt=np.loadtxt('UCI_HAR_DATASET/train/Inertial Signals/total_acc_x_train.txt')\n",
        "yyt=np.loadtxt('UCI_HAR_DATASET/train/Inertial Signals/total_acc_y_train.txt')\n",
        "zzt=np.loadtxt('UCI_HAR_DATASET/train/Inertial Signals/total_acc_z_train.txt')\n",
        "xxg=np.loadtxt('UCI_HAR_DATASET/train/Inertial Signals/body_gyro_x_train.txt')\n",
        "yyg=np.loadtxt('UCI_HAR_DATASET/train/Inertial Signals/body_gyro_y_train.txt')\n",
        "zzg=np.loadtxt('UCI_HAR_DATASET/train/Inertial Signals/body_gyro_z_train.txt')\n",
        "# concatenate into a single (n,128,9) array\n",
        "xtrain_all = np.concatenate((xx[:,:,None],yy[:,:,None],zz[:,:,None],\n",
        "xxt[:,:,None],yyt[:,:,None],zzt[:,:,None],\n",
        "xxg[:,:,None],yyg[:,:,None],zzg[:,:,None]),axis=2)\n",
        "\n",
        "# Splitting train data\n",
        "x_train_all, x_val_all = train_test_split(xtrain_all, test_size = 0.2, random_state=42)"
      ],
      "id": "f480c3af-d480-4cbe-9c2d-27fa30ce069b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff7c9e12-991f-46c1-bd85-c72cd3fb11fe"
      },
      "source": [
        "### Using best model configuration from Part 2 with all 9 variables"
      ],
      "id": "ff7c9e12-991f-46c1-bd85-c72cd3fb11fe"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b9cff486-2995-4c5b-bc6a-3167dc35399c"
      },
      "outputs": [],
      "source": [
        "# Best perfoming configuration from part 2 using all 9 variables\n",
        "# Remove the previous log folder\n",
        "!rm -rf ./tflogs/\n",
        "# Parameters\n",
        "kernal = 4\n",
        "filters = 64\n",
        "lr = 0.0001\n",
        "batch_size = 64\n",
        "epochs = 2000\n",
        "\n",
        "# Constructing CNN model\n",
        "cnn2 = tf.keras.Sequential()\n",
        "cnn2.add(tf.keras.layers.Conv1D(filters = filters, kernel_size = kernal, input_shape = (128, 9)))\n",
        "cnn2.add(tf.keras.layers.BatchNormalization())\n",
        "cnn2.add(tf.keras.layers.ReLU())\n",
        "cnn2.add(tf.keras.layers.GlobalAveragePooling1D())\n",
        "cnn2.add(tf.keras.layers.Dense(units = 6, activation = tf.nn.softmax))\n",
        "\n",
        "# Set up logging\n",
        "logdir = 'tflogs'\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
        "\n",
        "# creating callback to save best model\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.ModelCheckpoint(\"best_model_cnn2\", save_best_only=True, monitor=\"val_accuracy\",\n",
        "                                       mode='max', verbose = 0),\n",
        "    tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
        "]\n",
        "\n",
        "# Compiling\n",
        "cnn2.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = lr),\n",
        "\t\t\tloss='categorical_crossentropy', metrics=['accuracy'])\n",
        "# Fitting\n",
        "cnn2.fit(x_train_all, y_train1h, epochs = epochs, batch_size = batch_size, verbose = 0,\n",
        "        validation_data=(x_val_all, y_val1h), callbacks=callbacks)\n",
        "# recording best model configuration\n",
        "net3_1 = tf.keras.models.load_model(\"best_model_cnn2\")\n",
        "# Training accuracy\n",
        "print(\"Training loss and accuracy:\", net3_1.evaluate(x_train_all, y_train1h))\n",
        "# Validation accuracy\n",
        "print(\"Validation loss and accuracy:\", net3_1.evaluate(x_val_all, y_val1h))"
      ],
      "id": "b9cff486-2995-4c5b-bc6a-3167dc35399c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffbd416a-f627-4f5f-b43f-8e331a5f9d08"
      },
      "source": [
        "### Trying model 3 with multiple convolutional layers"
      ],
      "id": "ffbd416a-f627-4f5f-b43f-8e331a5f9d08"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DnzTxoqHBoPU"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir /content/tflogs"
      ],
      "id": "DnzTxoqHBoPU"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23c17476-e38c-4ce4-942c-1fdb3e76398f"
      },
      "outputs": [],
      "source": [
        "# Remove the previous log folder\n",
        "!rm -rf ./tflogs/\n",
        "# Parameters\n",
        "lr = 0.0001\n",
        "batch_size = 64\n",
        "epochs = 2000\n",
        "\n",
        "# Constructing CNN model\n",
        "cnn2 = tf.keras.Sequential()\n",
        "cnn2.add(tf.keras.layers.Conv1D(filters = 16, kernel_size = 3, input_shape = (128, 9)))\n",
        "cnn2.add(tf.keras.layers.BatchNormalization())\n",
        "cnn2.add(tf.keras.layers.Conv1D(filters = 32, kernel_size = 4))\n",
        "cnn2.add(tf.keras.layers.BatchNormalization())\n",
        "cnn2.add(tf.keras.layers.Conv1D(filters = 64, kernel_size = 5))\n",
        "cnn2.add(tf.keras.layers.BatchNormalization())\n",
        "cnn2.add(tf.keras.layers.ReLU())\n",
        "cnn2.add(tf.keras.layers.GlobalAveragePooling1D())\n",
        "cnn2.add(tf.keras.layers.Dense(units = 6, activation = tf.nn.softmax))\n",
        "\n",
        "# Set up logging\n",
        "logdir = 'tflogs'\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
        "\n",
        "# creating callback to save best model\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.ModelCheckpoint(\"best_model_cnn2\", save_best_only=True, monitor=\"val_accuracy\",\n",
        "                                       mode='max', verbose = 0),\n",
        "    tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
        "]\n",
        "\n",
        "# Compiling\n",
        "cnn2.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = lr),\n",
        "\t\t\tloss='categorical_crossentropy', metrics=['accuracy'])\n",
        "# Fitting\n",
        "cnn2.fit(x_train_all, y_train1h, epochs = epochs, batch_size = batch_size, verbose = 0,\n",
        "        validation_data=(x_val_all, y_val1h), callbacks=callbacks)\n",
        "# recording best model configuration\n",
        "net3_2 = tf.keras.models.load_model(\"best_model_cnn2\")\n",
        "# Training accuracy\n",
        "print(\"Training loss and accuracy:\", net3_2.evaluate(x_train_all, y_train1h))\n",
        "# Validation accuracy\n",
        "print(\"Validation loss and accuracy:\", net3_2.evaluate(x_val_all, y_val1h))"
      ],
      "id": "23c17476-e38c-4ce4-942c-1fdb3e76398f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cae6258a-ad9f-4890-bbf8-0768f08a8be2"
      },
      "outputs": [],
      "source": [
        "# Model from Part 3 test accuracy\n",
        "print(\"Test loss and accuracy:\", net3_2.evaluate(xtest_all, y_test1h))"
      ],
      "id": "cae6258a-ad9f-4890-bbf8-0768f08a8be2"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}